{"cells":[{"metadata":{},"cell_type":"markdown","source":"This was working, then i reset the runtime and now getting a broadcasting error. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, numpy as np, scipy as sp, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, pymc3 as pm, arviz as az\nplt.style.use(\"seaborn-darkgrid\")\nnp.seterr(divide='ignore')\n\ntune  = 2   # number of tuning steps before we start keeping good data\ncores = 4     # kaggle kernels have 4 cores, so we can use each one to run chains in parallel\neps   = 0.2   # we want prior distributions centered near, but not right at, the true value.  eps pushes the prior away from true\nseed  = 42\nrefresh = False  \n# refresh = True  # Delete & recreate any existing data\ndraws = 4 * tune\ndraws_total = draws * cores","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initial conditions - p.126\ntimes = np.arange(0, 22)\nclasses_HIV = ['S','I1','I2','I3','I4']\nclasses_SIR = ['S','I']\ny0_HIV = [6992.7, 7.3 , 0, 0,0]\ny0_SIR = [6992.7, 7.3]\nN = np.sum(y0_HIV)\n\n\n# published parameter point estimates - p.129\ntheta = {'alpha': 235.567,'beta': 0.332276 , 'gamma': 0.349035}\nn_theta = len(theta)\n#obs_SIR = [7.3,9.2,11.3,13.5,15.9,18.3,20.6,22.7,24.6,26.3,27.8,29.0,\n#          30.0,30.8,31.4,31.9,32.4,32.8,33.4,33.3,34.0,34.0] \n\n# normalize parameters - beta must be multiplied by N, all others unchanged\ntheta_n = theta.copy()\n#theta_n['beta'] *= N\ntheta_n['sigma'] = 1.0\n\ntheta_loc = {key: val * np.random.default_rng(seed).uniform(1-eps, 1+eps) for key,val in theta_n.items()}\n\n# ODE model - p.126\ndef ODE_HIV(y, t, p): \n    Lambda = 100\n    mu = 1/70\n    i_s = y[1]+y[2]+y[3]+y[4]\n    n_s = y[0]+y[1]+y[2]+y[3]+y[4] \n    lamb = p[1]*np.exp(-p[0]*i_s /n_s) * i_s / n_s\n    \n    ds = Lambda - lamb * y[0] - mu *    y[0]\n    di1 = lamb * y[0] - (p[2] + mu)* y[1]\n    di2 = p[2] * y[1] - (p[2] + mu)* y[2]\n    di3 = p[2] * y[2] - (p[2] + mu)* y[3]\n    di4 = p[2] * y[3] - (p[2] + mu)* y[4]\n    return [ds, di1, di2, di3, di4]\n\ndef ODE_SIR(y, t, p):\n    Lambda = 100\n    mu = 1/70\n    n_s = y[0]+y[1]\n    lamb = p[1]*np.exp(-p[0]*y[1] /n_s) * y[1] / n_s  \n    ds = Lambda - lamb * y[0] - mu * y[0]\n    di = lamb * y[0] - mu * y[1] \n    return [ds, di]\n\n# y_SIR, y_HIV, obs_SIR, obs_HIV, z_SIR, and z_HIV. Each _SIR has 2 classes \n#(one $S$ & one $I$) and each _HIV has 5 classes (one $S$ & four $I_j$).\n# I have 2 functions to convert between these variables.\n\ndef HIV_to_SIR(hiv): \n    return np.vstack([hiv[0],np.sum(hiv[1:])])\n\ndef SIR_to_HIV(sir):\n    I = y_HIV[1:]\n    T = np.sum(y_HIV[1:])\n    K = I/T \n    return np.vstack([sir[0], sir[1]*K])\n\ndef inject_noise(noise, seed, obs):\n    \"\"\"Add noise to S class predictions from odeint\"\"\"\n    z = obs.copy()\n    z[0] = np.random.default_rng(seed).lognormal(mean=np.log(z[0]), sigma=noise)\n    return z\n\ndef plot_data(z,y, classes):\n    colors = ['red', 'green', 'orange', 'blue', 'yellow', 'violet']\n    d = {'marker':'+', 'markersize':5, 'linestyle':''}\n    for i in range(0, 2):\n        clr = colors.pop(0)\n        plt.plot(times, y[i], c=clr, label=classes[i])\n        plt.plot(times, z[i], c=clr, label=None, **d)\n    plt.legend()\n    plt.show()\n    \ny_SIR = sp.integrate.odeint(ODE_SIR, y0_SIR, times, (list(theta.values()),), rtol=1e-8)\ny_HIV = sp.integrate.odeint(ODE_HIV, y0_HIV, times, (list(theta.values()),), rtol=1e-8)\ny_SIR = y_SIR.T/N\ny_HIV = y_HIV.T/N\n\n  # get ODE predictions\ny_HIV = y_HIV.T / N  # normalize & transpose\n# incorporate observations in Table 6.1 p.126\nobs_SIR = y_SIR.copy()\nobs_SIR[1,0:] = np.array([7.3,9.2,11.3,13.5,15.9,18.3,20.6,22.7,24.6,26.3,27.8,29.0,30.0,30.8,31.4,31.9,32.4,32.8,33.4,33.3,34.0,34.0]) / N\nobs_HIV = SIR_to_HIV(obs_SIR)","execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"operands could not be broadcast together with shapes (22,) (21,5) ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-688d70ae904a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mobs_SIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_SIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mobs_SIR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m27.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m29.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m33.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m33.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mobs_HIV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIR_to_HIV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_SIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-688d70ae904a>\u001b[0m in \u001b[0;36mSIR_to_HIV\u001b[0;34m(sir)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_HIV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minject_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (22,) (21,5) "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize parameters - beta must be multiplied by N, all others unchanged\ntheta_n = theta.copy()\n#theta_n['beta'] *= N\ntheta_n['sigma'] = 1.0\n\n# guess loc for priors, but inject some error so Bayes doesn't get to start at the correct answer\ntheta_loc = {key: val * np.random.default_rng(seed).uniform(1-eps, 1+eps) for key,val in theta_n.items()}\n\n# create pymc3 ODE object\nODE_pymc3_HIV = pm.ode.DifferentialEquation(\n    func=ODE_HIV,\n    times=times[1:],\n    t0=times[0],\n    n_states=len(classes_HIV),\n    n_theta=n_theta)\n\nODE_pymc3_SIR = pm.ode.DifferentialEquation(\n    func=ODE_SIR,\n    times=times[1:],\n    t0=times[0],\n    n_states=len(classes_SIR),\n    n_theta=n_theta)\n\n\ndef make_model_HIV(z, classes):\n    \"\"\"Creates the pymc3 model object, but does not run it\"\"\"\n    with pm.Model() as model:\n        # Choose priors on alpha, beta, and sigma\n        sigma_prior = pm.HalfCauchy(\"sigma\", theta_loc['sigma'], shape=len(classes))\n        params = theta_loc.copy()\n        params.pop('sigma')\n        theta_prior = [pm.Lognormal(key, mu=pm.math.log(loc), sigma=1.0) for key,loc in params.items()]\n        curves = ODE_pymc3_HIV(y0=z.T[0], theta=theta_prior)\n        Z = pm.Lognormal(\"Z\", mu=pm.math.log(curves), sigma=sigma_prior, observed=z.T[1:])\n    return model\n\ndef make_model_SIR(z, classes):\n    \"\"\"Creates the pymc3 model object, but does not run it\"\"\"\n    with pm.Model() as model:\n        # Choose priors on alpha, beta, and sigma\n        sigma_prior = pm.HalfCauchy(\"sigma\", theta_loc['sigma'], shape=len(classes))\n        params = theta_loc.copy()\n        params.pop('sigma')\n        theta_prior = [pm.Lognormal(key, mu=pm.math.log(loc), sigma=1.0) for key,loc in params.items()]\n        curves = ODE_pymc3_SIR(y0=z.T[0], theta=theta_prior)\n        Z = pm.Lognormal(\"Z\", mu=pm.math.log(curves), sigma=sigma_prior, observed=z.T[1:])\n    return model\n\ndef analyze(idata,y,z, classes):\n    \"\"\"Analyzes results of pymc3 run\"\"\"\n    # Compute stats on parameters posterior distributions\n    theta_stats = idata.posterior.to_dataframe().describe().T  # 7-number summaries\n    theta_stats['draws'] = idata.posterior.beta.size  # count\n    theta_stats['TRUE'] = pd.Series(theta_n)          # true value from p.126\n    theta_stats['prior_loc'] = pd.Series(theta_loc)   # start value for bayes\n    theta_stats = theta_stats[['draws', 'min', '25%', '50%', 'TRUE', 'mean', '75%', 'max', 'prior_loc']]  # rearrange columns\n    display(theta_stats)\n    az.plot_posterior(idata, round_to=4, hdi_prob=0.95)  # plot posteriors\n    plt.show()\n    \n    # plot posterior predictive data (aka - draw parameters from their posterior and run ODE with them)\n    pred = idata.posterior_predictive.to_dataframe().reset_index().rename(columns={'Z_dim_0':'time', 'Z_dim_1':'class'})\n    pred['time'] += times[1]  # replace time=0 with time=first observations\n    pred = pred.pivot(['chain','draw','class'], 'time', 'Z')  # convert from \"long form\" to \"wide form\" with column for each time\n    t0 = times[0]\n    pred[t0] = 0.0  # create placeholder column for initial condition, to be filled in later\n    pred = pred[sorted(pred.columns)]  # sort columns in order\n    r = len(classes)\n    fig, axes = plt.subplots(r, 2, figsize=(12, 4*r))  # create grid of plots\n    i = -1  # current row of plot grid\n    for c, df in pred.groupby('class'):  # make plot for each class\n        i += 1\n        data = df.reset_index(drop=True)\n        data[t0] = y[c,0]  # fill initial condition columns created above\n        for j in range(2):  # create boxplot and lineplot side-by-side\n            ax = axes[i,j]\n            if j == 0:\n                # dt is a silly hack to make boxplot overlay correctly with ODE solution & observed data\n                dt = t0\n                sns.boxplot(data=data, showfliers=False, ax=ax)  # posterior predictive as boxplots\n                # find good lower and upper ylim\n                l = max(ax.get_ylim()[0], 0.0)\n                u = min(ax.get_ylim()[1], 1.1)\n            elif j == 1:\n                # lineplot does not need the same silly hack\n                dt = 0\n                # lineplot takes a long time, so we limit the number of posterior predictions to show\n                num_plots = 300\n                sns.lineplot(data=data.sample(num_plots).T, linewidth=0.5, alpha=0.9, legend=False, ax=ax)  # posterior predictive data as lines\n            ax.plot(times-dt, y[c], color='red' , linewidth=3)  # plot ODE solution\n            ax.plot(times-dt, z[c], color='blue', marker='+', markersize=5, linestyle='')  # plot observed data\n            ax.set_xticks(times-dt)\n            ax.tick_params('x', labelrotation=90)\n            ax.set_xlabel('time')\n            ax.xaxis.grid(False)\n            ax.yaxis.grid(True)\n            ax.set_ylabel(classes[c])\n            ax.set_ylim(l, u)  # use l, u range from boxplot\n    plt.tight_layout()\n    plt.show()\n    return theta_stats\n\ndef del_file(file):\n    \"\"\" delete file if it exists\"\"\"\n    try:\n        os.remove(file)\n    except OSError:\n        pass\n\n# noises = [0.01, 0.02, 0.05, 0.1]\nnoises = [0.02]\nfor noise in sorted(noises): # amount of noise to inject into ode-solved values & day to start observed data    \n    print(f'\\n\\nFor noise={noise} draws_total={draws_total}', end=' ... ')\n    file = f'/home/jupyter/idata/bayes_noise={int(noise*100)%100}_draws={draws_total}.netcdf'  # file to read & write data\n    z_SIR = inject_noise(noise, seed, obs_SIR)\n    plot_data(z_SIR,y_SIR ,classes_SIR)\n    model1 = make_model_SIR(z_SIR, classes_SIR)\n    with model1:\n        try:\n            assert not refresh  # if refresh true, go to except block and re-create\n            idata = az.from_netcdf(file)  # try to read existing data, if fail, go to except block and re-create\n            print(f'using idata from {file}')\n        except:\n            # The big, intense line is below.  It runs MCMC and can take a long time\n            # tune = # steps to throw away at the beginning (bad data)\n            # draws = # steps to keep\n            # cores = # cores in machine - use them all\n            # chains = cores to use all available power\n            print('Sampling posterior')\n            idata = pm.sample(tune=tune, draws=draws, cores=cores, chains=cores, target_accept=0.9, return_inferencedata=True)\n            # save to file for reuse\n            del_file(file)\n            idata.to_netcdf(file)\n        # Now we have the distribution for parameters.  We'd like to see what that means for S & I evolutions\n        # pymc3 calls this \"sample posterior predictive\"\n        # If we're using existing data, it may have posterior_predictive already.  Else, generate\n        if not hasattr(idata, 'posterior_predictive'):\n            # Draw samples\n            print('Sampling posterior_predictive')\n            ppc = pm.sample_posterior_predictive(idata, keep_size=True)\n            # put into idata\n            az.concat(idata, az.from_dict(posterior_predictive=ppc), inplace=True)\n            # delete old file and save new one\n            del_file(file)\n            idata.to_netcdf(file)\n        analyze(idata,y_HIV,z_HIV, classes_SIR)","execution_count":3,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"operands could not be broadcast together with shapes (22,) (21,5) ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-7502e1c6fb29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mobs_SIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_SIR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mobs_SIR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m27.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m29.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m31.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m33.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m33.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mobs_HIV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIR_to_HIV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_SIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-7502e1c6fb29>\u001b[0m in \u001b[0;36mSIR_to_HIV\u001b[0;34m(sir)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_HIV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minject_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (22,) (21,5) "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for noise in sorted(noises): # amount of noise to inject into ode-solved values & day to start observed data    \n    print(f'\\n\\nFor noise={noise} draws_total={draws_total}', end=' ... ')\n    file = f'/home/jupyter/idata/bayes_noise={int(noise*100)%100}_draws={draws_total}.netcdf'  # file to read & write data\n    z_HIV = inject_noise(noise, seed, obs_HIV)\n    plot_data(z_HIV,y_HIV ,classes_HIV)\n    model2 = make_model_HIV(z_HIV, classes_HIV)\n    with model2:\n        try:\n            assert not refresh  # if refresh true, go to except block and re-create\n            idata = az.from_netcdf(file)  # try to read existing data, if fail, go to except block and re-create\n            print(f'using idata from {file}')\n        except:\n            # The big, intense line is below.  It runs MCMC and can take a long time\n            # tune = # steps to throw away at the beginning (bad data)\n            # draws = # steps to keep\n            # cores = # cores in machine - use them all\n            # chains = cores to use all available power\n            print('Sampling posterior')\n            idata = pm.sample(tune=tune, draws=draws, cores=cores, chains=cores, target_accept=0.9, return_inferencedata=True)\n            # save to file for reuse\n            del_file(file)\n            idata.to_netcdf(file)\n        # Now we have the distribution for parameters.  We'd like to see what that means for S & I evolutions\n        # pymc3 calls this \"sample posterior predictive\"\n        # If we're using existing data, it may have posterior_predictive already.  Else, generate\n        if not hasattr(idata, 'posterior_predictive'):\n            # Draw samples\n            print('Sampling posterior_predictive')\n            ppc = pm.sample_posterior_predictive(idata, keep_size=True)\n            # put into idata\n            az.concat(idata, az.from_dict(posterior_predictive=ppc), inplace=True)\n            # delete old file and save new one\n            del_file(file)\n            idata.to_netcdf(file)\n        analyze(idata,y_HIV, z_HIV, classes_HIV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, numpy as np, scipy as sp, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, pymc3 as pm, arviz as az\nplt.style.use(\"seaborn-darkgrid\")\nnp.seterr(divide='ignore')\n\ntune  = 500   # number of tuning steps before we start keeping good data\ncores = 4     # kaggle kernels have 4 cores, so we can use each one to run chains in parallel\neps   = 0.2   # we want prior distributions centered near, but not right at, the true value.  eps pushes the prior away from true\nseed  = 42\nrefresh = False  \n# refresh = True  # Delete & recreate any existing data\ndraws = 4 * tune\ndraws_total = draws * cores\n\n# initial conditions - p.126\ntimes = np.arange(1, 15)\nclasses = ['S','I']\ny0 = [760, 3]\nN = np.sum(y0)\n\n# published parameter point estimates - p.129\ntheta = {'beta' : 0.00237, 'alpha': 0.465}\nn_theta = len(theta)\n\n# ODE model - p.126\ndef ODE(y, t, p):\n    ds = -p[0] * y[0] * y[1]\n    di =  p[0] * y[0] * y[1] - p[1] * y[1]\n    return [ds, di]\n\ndef inject_noise(noise, seed):\n    \"\"\"Add noise to S class predictions from odeint\"\"\"\n    z = obs.copy()\n    z[0] = np.random.default_rng(seed).lognormal(mean=np.log(z[0]), sigma=noise)\n    return z\n\ndef plot_data(z):\n    colors = ['red', 'green', 'orange', 'blue', 'yellow', 'violet']\n    d = {'marker':'+', 'markersize':5, 'linestyle':''}\n    for i in range(0, 2):\n        clr = colors.pop(0)\n        plt.plot(times, y[i], c=clr, label=classes[i])\n        plt.plot(times, z[i], c=clr, label=None, **d)\n    plt.legend()\n    plt.show()\n    \n# ODE model in book works on raw counts, but PyMC3 does better with proportions\n# So, we will run initially create and run the raw version to get ODE solutions,\n# but then \"normalize\" to proportions to run Bayes.\n# This may require adjustement to both the variables and parameters, as discussed in lecture.\n\n# Run ODE on counts, then normalize to propotions\ny = sp.integrate.odeint(ODE, y0, times, (list(theta.values()),), rtol=1e-8)  # get ODE predictions\ny = y.T / N  # normalize & transpose\n# incorporate observations in Table 6.1 p.126\nobs = y.copy()\nobs[1, 2:] = np.array([25,75,227,296,258,236,192,126,71,28,11,7]) / N\n\nobs.shape()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize parameters - beta must be multiplied by N, all others unchanged\ntheta_n = theta.copy()\ntheta_n['beta'] *= N\ntheta_n['sigma'] = 1.0\n\n# guess loc for priors, but inject some error so Bayes doesn't get to start at the correct answer\ntheta_loc = {key: val * np.random.default_rng(seed).uniform(1-eps, 1+eps) for key,val in theta_n.items()}\n\n# create pymc3 ODE object\nODE_pymc3 = pm.ode.DifferentialEquation(\n    func=ODE,\n    times=times[1:],\n    t0=times[0],\n    n_states=len(classes),\n    n_theta=n_theta)\n\ndef make_model(z=obs):\n    \"\"\"Creates the pymc3 model object, but does not run it\"\"\"\n    with pm.Model() as model:\n        # Choose priors on alpha, beta, and sigma\n        sigma_prior = pm.HalfCauchy(\"sigma\", theta_loc['sigma'], shape=len(classes))\n        params = theta_loc.copy()\n        params.pop('sigma')\n        theta_prior = [pm.Lognormal(key, mu=pm.math.log(loc), sigma=1.0) for key,loc in params.items()]\n        curves = ODE_pymc3(y0=z.T[0], theta=theta_prior)\n        Z = pm.Lognormal(\"Z\", mu=pm.math.log(curves), sigma=sigma_prior, observed=z.T[1:])\n    return model\n\ndef analyze(idata):\n    \"\"\"Analyzes results of pymc3 run\"\"\"\n    # Compute stats on parameters posterior distributions\n    theta_stats = idata.posterior.to_dataframe().describe().T  # 7-number summaries\n    theta_stats['draws'] = idata.posterior.beta.size  # count\n    theta_stats['TRUE'] = pd.Series(theta_n)          # true value from p.126\n    theta_stats['prior_loc'] = pd.Series(theta_loc)   # start value for bayes\n    theta_stats = theta_stats[['draws', 'min', '25%', '50%', 'TRUE', 'mean', '75%', 'max', 'prior_loc']]  # rearrange columns\n    display(theta_stats)\n    az.plot_posterior(idata, round_to=4, hdi_prob=0.95)  # plot posteriors\n    plt.show()\n    \n    # plot posterior predictive data (aka - draw parameters from their posterior and run ODE with them)\n    pred = idata.posterior_predictive.to_dataframe().reset_index().rename(columns={'Z_dim_0':'time', 'Z_dim_1':'class'})\n    pred['time'] += times[1]  # replace time=0 with time=first observations\n    pred = pred.pivot(['chain','draw','class'], 'time', 'Z')  # convert from \"long form\" to \"wide form\" with column for each time\n    t0 = times[0]\n    pred[t0] = 0.0  # create placeholder column for initial condition, to be filled in later\n    pred = pred[sorted(pred.columns)]  # sort columns in order\n    r = len(classes)\n    fig, axes = plt.subplots(r, 2, figsize=(12, 4*r))  # create grid of plots\n    i = -1  # current row of plot grid\n    for c, df in pred.groupby('class'):  # make plot for each class\n        i += 1\n        data = df.reset_index(drop=True)\n        data[t0] = y[c,0]  # fill initial condition columns created above\n        for j in range(2):  # create boxplot and lineplot side-by-side\n            ax = axes[i,j]\n            if j == 0:\n                # dt is a silly hack to make boxplot overlay correctly with ODE solution & observed data\n                dt = t0\n                sns.boxplot(data=data, showfliers=False, ax=ax)  # posterior predictive as boxplots\n                # find good lower and upper ylim\n                l = max(ax.get_ylim()[0], 0.0)\n                u = min(ax.get_ylim()[1], 1.1)\n            elif j == 1:\n                # lineplot does not need the same silly hack\n                dt = 0\n                # lineplot takes a long time, so we limit the number of posterior predictions to show\n                num_plots = 300\n                sns.lineplot(data=data.sample(num_plots).T, linewidth=0.5, alpha=0.9, legend=False, ax=ax)  # posterior predictive data as lines\n            ax.plot(times-dt, y[c], color='red' , linewidth=3)  # plot ODE solution\n            ax.plot(times-dt, z[c], color='blue', marker='+', markersize=5, linestyle='')  # plot observed data\n            ax.set_xticks(times-dt)\n            ax.tick_params('x', labelrotation=90)\n            ax.set_xlabel('time')\n            ax.xaxis.grid(False)\n            ax.yaxis.grid(True)\n            ax.set_ylabel(classes[c])\n            ax.set_ylim(l, u)  # use l, u range from boxplot\n    plt.tight_layout()\n    plt.show()\n    return theta_stats\n\ndef del_file(file):\n    \"\"\" delete file if it exists\"\"\"\n    try:\n        os.remove(file)\n    except OSError:\n        pass\n\n# noises = [0.01, 0.02, 0.05, 0.1]\nnoises = [0.02]\nfor noise in sorted(noises): # amount of noise to inject into ode-solved values & day to start observed data    \n    print(f'\\n\\nFor noise={noise} draws_total={draws_total}', end=' ... ')\n    file = f'/home/jupyter/idata/bayes_noise={int(noise*100)%100}_draws={draws_total}.netcdf'  # file to read & write data\n    z = inject_noise(noise, seed)\n    plot_data(z)\n    model = make_model(z)\n    with model:\n        try:\n            assert not refresh  # if refresh true, go to except block and re-create\n            idata = az.from_netcdf(file)  # try to read existing data, if fail, go to except block and re-create\n            print(f'using idata from {file}')\n        except:\n            # The big, intense line is below.  It runs MCMC and can take a long time\n            # tune = # steps to throw away at the beginning (bad data)\n            # draws = # steps to keep\n            # cores = # cores in machine - use them all\n            # chains = cores to use all available power\n            print('Sampling posterior')\n            idata = pm.sample(tune=tune, draws=draws, cores=cores, chains=cores, target_accept=0.9, return_inferencedata=True)\n            # save to file for reuse\n            del_file(file)\n            idata.to_netcdf(file)\n        # Now we have the distribution for parameters.  We'd like to see what that means for S & I evolutions\n        # pymc3 calls this \"sample posterior predictive\"\n        # If we're using existing data, it may have posterior_predictive already.  Else, generate\n        if not hasattr(idata, 'posterior_predictive'):\n            # Draw samples\n            print('Sampling posterior_predictive')\n            ppc = pm.sample_posterior_predictive(idata, keep_size=True)\n            # put into idata\n            az.concat(idata, az.from_dict(posterior_predictive=ppc), inplace=True)\n            # delete old file and save new one\n            del_file(file)\n            idata.to_netcdf(file)\n        analyze(idata)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}